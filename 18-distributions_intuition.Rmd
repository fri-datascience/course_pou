# Distributions intutition

This chapter is intended to help you familiarize yourself with the different 
probability distributions you will encounter in this course.

Use [Appendix B](#distributions) as a reference for the basic properties of distributions. 


```{r, echo = FALSE}
togs   <- T
tog_ex <- T
```



<style>
.fold-btn { 
  float: right; 
  margin: 5px 5px 0 0;
}
.fold { 
  border: 1px solid black;
  min-height: 40px;
}
</style>

<script type="text/javascript">
$(document).ready(function() {
  $folds = $(".fold");
  $folds.wrapInner("<div class=\"fold-blck\">"); // wrap a div container around content
  $folds.prepend("<button class=\"fold-btn\">Unfold Solution</button>");  // add a button
  $(".fold-blck").toggle();  // fold all blocks
  $(".fold-btn").on("click", function() {  // add onClick event
    $(this).text($(this).text() === "Fold Solution" ? "Unfold Solution" : "Fold Solution");  // if the text equals "Fold", change it to "Unfold"or else to "Fold" 
    $(this).next(".fold-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
  })
});
</script>


## Discrete distributions

```{exercise, name = "Bernoulli intuition 1"}
Arguably the simplest distribution you will enocounter is the Bernoulli distribution.
It is a discrete probability distribution used to represent the outcome of a yes/no
question. It has one parameter $p$ which is the probability of success. The 
probability of failure is $1-p$, sometimes denoted as $q$.

A classic way to think about a Bernoulli trial (a yes/no experiment) is a coin 
flip. Real coins are fair, meaning the probability of either heads (1) 
or tails (0) are the same i.e. $p=0.5$, shown below in *figure a*. Alternatively
we may want to represent a process that doesn't have equal probabilities of outcomes
like "Will a throw of a fair die result in a 6?". In this case $p=\frac{1}{6}$, 
shown in *figure b*.

Using your knowledge of the Bernoulli distribution use the throw of a fair die 
to think of events, such that:

a) $p = 0.5$
b) $p = \frac{5}{6}$
c) $q = \frac{2}{3}$
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)

# Create data
df <- data.frame(
  outcome = rep(c(0, 1), 2),
  probability = c(1-0.5, 0.5, 1-1/6, 1/6),
  p_value = factor(rep(c("a", "b"), each=2))
)

# Plot
ggplot(df, aes(x=factor(outcome), y=probability)) +
  geom_segment(aes(xend=factor(outcome), yend=0), color='black') +
  geom_point(color="red", size=3) +
  coord_cartesian(ylim = c(0, 1)) +
  labs(y = "Probability", x = "Outcome") +
  theme(legend.position="none") +
  facet_wrap(~p_value, ncol=2, scales = "free_x", labeller = label_parsed)

```

<div class="fold">
```{solution, echo = togs}
a. An event that is equally likely to happen or not happen i.e. $p = 0.5$ would be
throwing an even number. More formally we can name this event $A$ and write:
  $A = \{2,4,6\}$, $P(A) = 0.5$


b. An example of an event with $p = \frac{5}{6}$ would be throwing a number 
greater than 1. Defined as $B = \{2,3,4,5,6\}$.

c. We need an event that fails $\frac{2}{3}$ of the time. Alternatively we can 
reverse the problem and find an event that succeeds $\frac{1}{3}$ of the time, 
since: $q = 1 - p \implies p = 1 - q = \frac{1}{3}$. The event that our outcome
is divisible by 3: $C = \{3, 6\}$ satisfies this condition.
```
</div>

```{exercise, name = "Binomial intuition 1"}
The binomial distribution is a generalization of the Bernoulli distribution. 
Instead of considering a single Bernoulli trial, we now consider a sequence of $n$ trials,
which are independent and have the same parameter $p$. So the binomial distribution
has two parameters $n$ - the number of trials and $p$ - the probability of success
for each trial.

If we return to our coin flip representation, we now flip a coin several times.
The binomial distribution will give us the probabilities of all possible outcomes.
Below we show the distribution for a series of 10 coin flips with a fair coin 
(left) and a biased coin (right). The numbers on the x axis represent the 
number of times the coin landed heads.

Using your knowledge of the binomial distribution:

a. Take the [pmf of the binomial distribution](#distributions)  and plug in $n=1$,
check that it is in fact equivalent to a Bernoulli distribution.

b. In our examples we show the graph of a binomial distribution over 10 trials with
$p=0.8$. If we take a look at the graph, it appears as though the probabilities of getting 0,1,2 or 3
heads in 10 flips are zero. Is it actually zero? Check by plugging in the values
into the pmf.
```

<div class="fold">
```{solution, echo = togs}
a. The pmf of a binomial distribution is $\binom{n}{k} p^k (1 - p)^{n - k}$, now
we insert $n=1$ to get: 
  $$\binom{1}{k} p^k (1 - p)^{1 - k}$$.
Not quite equivalent to
a Bernoulli, however note that the support of the binomial distribution is 
defined as $k \in \{0,1,\dots,n\}$, so in our case $k = \{0,1\}$, then:
  $$\binom{1}{0} = \binom{1}{1} = 1$$ 
  we get:  $p^k (1 - p)^{1 - k}$ ,the Bernoulli distribution.

b. As we already know $p=0.8, n=10$, so:
  $$\binom{10}{0} 0.8^0 (1 - 0.8)^{10 - 0} = 1.024 \cdot 10^{-7}$$
  $$\binom{10}{1} 0.8^1 (1 - 0.8)^{10 - 1} = 4.096 \cdot 10^{-6}$$
  $$\binom{10}{2} 0.8^2 (1 - 0.8)^{10 - 2} = 7.3728 \cdot 10^{-5}$$
  $$\binom{10}{3} 0.8^3 (1 - 0.8)^{10 - 3} = 7.86432\cdot 10^{-4}$$
  So the probabilities are not zero, just very small.
```
</div>


```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)

# Data for binomial distribution
df1 <- data.frame(
  outcome = 0:10,
  probability = dbinom(0:10, size=10, prob=0.5),
  p_value = "p = 0.5"
)

df2 <- data.frame(
  outcome = 0:10,
  probability = dbinom(0:10, size=10, prob=0.8),
  p_value = "p = 0.8"
)

df <- rbind(df1, df2)

ggplot(df, aes(x=factor(outcome), y=probability)) +
  geom_segment(aes(xend=factor(outcome), yend=0), color='black') +
  geom_point(color="red", size=2) +
  labs(y = "Probability", x = "Outcome") +
  theme(legend.position="none") +
  facet_wrap(~p_value, ncol=2, scales = "free_x")

```


```{exercise, name = "Poisson intuition 1"}
Below are shown 3 different graphs of the Poisson distribution. Your task 
is to replicate them on your own in R by varying the $\lambda$ parameter.

Hint: You can use dpois() to get the probabilities.
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(gridExtra)

x = 0:15

# Create Poisson data
data1 <- data.frame(x = x, y = dpois(x, lambda = 0.1))
data2 <- data.frame(x = x, y = dpois(x, lambda = 1))
data3 <- data.frame(x = x, y = dpois(x, lambda = 7.5))

# Create individual ggplot objects
plot1 <- ggplot(data1, aes(x, y)) + geom_col() +
  xlab("x") + ylab("Probability") + ylim(0,1)

plot2 <- ggplot(data2, aes(x, y)) + geom_col() +
  xlab("x") + ylab(NULL) + ylim(0,1)

plot3 <- ggplot(data3, aes(x, y)) + geom_col() +
  xlab("x") + ylab(NULL) + ylim(0,1)

# Combine the plots
grid.arrange(plot1, plot2, plot3, ncol = 3)
```

<div class="fold">
```{r, echo = tog_ex, eval=FALSE}
library(ggplot2)
library(gridExtra)

x = 0:15

# Create Poisson data
data1 <- data.frame(x = x, y = dpois(x, lambda = 0.1))
data2 <- data.frame(x = x, y = dpois(x, lambda = 1))
data3 <- data.frame(x = x, y = dpois(x, lambda = 7.5))

# Create individual ggplot objects
plot1 <- ggplot(data1, aes(x, y)) + geom_col() +
  xlab("x") + ylab("Probability") + ylim(0,1)

plot2 <- ggplot(data2, aes(x, y)) + geom_col() +
  xlab("x") + ylab(NULL) + ylim(0,1)

plot3 <- ggplot(data3, aes(x, y)) + geom_col() +
  xlab("x") + ylab(NULL) + ylim(0,1)

# Combine the plots
grid.arrange(plot1, plot2, plot3, ncol = 3)
```
</div>


```{exercise, name = "Poisson intuition 2"}
The Poisson distribution is a discrete probability distribution that models 
processes where events occur at a constant mean rate and are independent of each other.

It has a single parameter $\lambda$, which represents the constant mean rate.

A classic example of a scenario that can be modeled using the Poisson distribution 
is the number of calls received by a call center in a day (or in fact any other
                                                           time interval).

Suppose you work in a call center and have some understanding of probability 
distributions. You overhear your supervisor mentioning that the call center 
receives an average of 2.5 calls per day. Using your knowledge of the Poisson 
distribution, calculate:
  
a. The probability you will get no calls today.
b. The probability you will get more than 5 calls today.
```

<div class="fold">
```{solution, echo = togs}

First recall the Poisson pmf: $$p(k) = \frac{\lambda^k e^{-\lambda}}{k!}$$
  
 as stated previously our parameter $\lambda = 2.5$

a. To get the probability of no calls we simply plug in $k = 0$, so: $$p(0) = \frac{2.5^0 e^{-2.5}}{0!} = e^{-2.5} \approx 0.082$$
  
b. The support of the Poisson distribution is non-negative integers. So if we wanted
to calculate the probability of getting more than 5 calls we would need to add up 
the probabilities of getting 6 calls and 7 calls and so on up to infinity.
Let us instead remember that the sum of all probabilties will be 1, we will
reverse the problem and instead ask "What is the probability we get 5 calls or less?".
We can subtract the probability of the opposite outcome (the complement) from 1
to get the probability of our original question.

 $$P(k > 5) = 1 - P(k \leq 5)$$
  $$P(k \leq 5) = \sum_{i=0}^{5} p(i) = p(0) + p(1) + p(2) + p(3) + p(4) + p(5) =$$
  $$= \frac{2.5^0 e^{-2.5}}{0!} + \frac{2.5^1 e^{-2.5}}{1!} + \dots =$$
  $$=0.957979$$

  So the probability of geting more than 5 calls will be $1 - 0.957979 = 0.042021$
```
</div>

```{exercise, name = "Geometric intuition 1"}
The geometric distribution is a discrete distribution that models the **number of
failures** before the first success in a sequence of independent Bernoulli trials.
It has a single parameter $p$, representing the probability of success.

*Disclaimer*: There are two forms of this distribution, the one we just described 
and another version that models the **number of trials** before the first success. The
difference is subtle yet significant and you are likely to encounter both forms, 
though here we will limit ourselves to the former.

In the graph below we show the pmf of a geometric distribution with $p=0.5$. This
can be thought of as the number of successive failures (tails) in the flip of a fair coin.
You can see that there's a 50% chance you will have zero failures i.e. you will 
flip a heads on your very first attempt. But there is some smaller chance that you
will flip a sequence of tails in a row, with longer sequences having ever lower 
probability.

Suppose you are gambling over coin flips with your friend and they propose if they 
get 5 tails in a row you must give them 100€ and you get 1€ if they fail.

a) Does it make sense to accept this wager?
b) They change their mind and claim that getting 10 tails in a row is half as 
likely, so now they want 200€ if they succeed, but still only offer to pay 1€
if they fail. Does it make sense to accept this wager?
c) *Bonus*: Look up the second form of this distribution and redo problems a) and b).

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)

# Parameters
p <- 0.5
x_vals <- 0:9  # Starting from 0
probs <- dgeom(x_vals, p)

# Data
data <- data.frame(x_vals, probs)

# Plot
ggplot(data, aes(x=x_vals, y=probs)) +
  geom_segment(aes(xend=x_vals, yend=0), color="black", size=1) +
  geom_point(color="red", size=2) + 
  labs(x = "Number of trials", y = "Probability") +
  theme_minimal() +
  scale_x_continuous(breaks = x_vals) # This line ensures integer x-axis labels

```

<div class="fold">
```{solution, echo = togs}
aaa
```
</div>

## Continuous distributions 
